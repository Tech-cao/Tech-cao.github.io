In this work, we present AuxLuxMuse, a novel system for automated aesthetic stage lighting generation that integrates expert knowledge, multimodal representation learning and preference-adaptive modeling. Lighting design in live performance settings requires the seamless translation of musical structure, rhythm and thematic intent into dynamic lighting behaviors. However, traditional workflows remain time-consuming, labor-intensive and difficult to transfer or scale. Mimicking the traditional stage lighting workflow, AuraLuxMuse replaces expert-driven analysis with deep learning-based feature extraction and decomposes lighting attributes into structured representations, which are jointly learned alongside music features under the supervision of curated metadata. At the heart of AuraLuxMuse are two key modules: LAMP (Lighting-Aligned Music Pretraining), which performs contrastive learning between audio and lighting cues to enable alignment, and PAMoE (Preference-Adaptive Mixture of Experts), which conditions lighting generation on designer intent through a gated ensemble of style-specific expert networks. To support training and evaluation, we introduce MusiLux, the first large-scale dataset of paired musical audio and professional lighting cue sequences under diverse live performance scenarios. Experimental results, including objective metrics and user study evaluation, demonstrate that AuxLuxMuse produces stage lighting that is visually cohesive, semantically aligned, and artistically expressive, advancing the frontier of AI-assisted aesthetic stage design.
